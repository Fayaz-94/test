{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a852f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "interviews :\n",
    "    \n",
    "    1. Resume prepartion \n",
    "    \n",
    "    2. Interview questions :\n",
    "        \n",
    "        1. self introduction\n",
    "        \n",
    "        2. project explanation /  explain me your project architecture.\n",
    "        \n",
    "        3. Roles & Resonsibiities as a Data engineer\n",
    "        \n",
    "        4. what are challenges you have faced while working as a data engineer\n",
    "        \n",
    "        5. what is your source and what is your target\n",
    "        \n",
    "        6. in which format are you getting the data\n",
    "        \n",
    "        7. for which client are you working as a Data engineer\n",
    "        \n",
    "        8. explain me your project architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389325e6",
   "metadata": {},
   "outputs": [],
   "source": [
    " 1. self introduction :\n",
    "        \n",
    "can you pls introduce your self :\n",
    "    \n",
    "    \n",
    "    Hi , My Name is ........... . I m having overall 4 + years of IT experience . I m  currently working with\n",
    "    \n",
    "    capgemini as a Data engineer. i m having relevant 2+ years of experience in to Bigdata. i have got experience in to the\n",
    "    \n",
    "    Bigdata technologies like Hadoop, HDFS, spark, hive , sqoop and airflow. i m having experience in python programming.\n",
    "    \n",
    "    also got the expsore on some of the Cloud services in AWS Like --AWS S3, AWS Glue, AWS Lambda etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29030480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef984bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. can you pls explain me about your project as Data engineer :\n",
    "    \n",
    "    \n",
    "I m currently working with : Adidas which is an e commerce company having its presence across the Globe\n",
    "    \n",
    "Project : Data migration from legacy systems to Bigdata platform\n",
    "    \n",
    "    \n",
    "various data sources :\n",
    "    \n",
    "    1. MYSQL DB -----------------------------> BDP ( Big data platform)\n",
    "    \n",
    "    2. Teradata -----------------------------> BDP ( Big Data platform)\n",
    "    \n",
    "    3. Salesforce CRM -----------------------> BDP ( Big Data platform)\n",
    "    \n",
    "    4. SAP HANA -----------------------------> BDP ( Big Data platform)\n",
    "    \n",
    "    5. POS ( point of sales)\n",
    "    \n",
    "    \n",
    "ETL ( Extract + transform + load )  =====> sqoop + airlflow for schelduling the JOBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7622484",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW Zone ( Landing Zone )  ( 1GB Data -------------> Transformation Zone ( 1GB )-----------> Semantic zone / Consumption zone( 1GB)\n",
    "\n",
    "HDFS / S3 Buckets -------------------> Hive + Spark Table ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03348d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hive is used to store the  data\n",
    "\n",
    "spark is used to the data transofrmation ( null handling , dupliocties removing , aggegrate operatoions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45c045d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d98b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "Semantic zone / Consumption zone :\n",
    "    \n",
    "It is final zone ====> here all the naming standards of the tables will be maintained and any column name addiing as per\n",
    "\n",
    "the requirements from the clients will be mainatined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bf0488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bec8bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Consumption zone =============> reporting team ( Tableau dashbaords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b369e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data retention policy -----> raw zone ( 30 days)\n",
    "\n",
    "transformation zone ( 60 days) =====> consumption zone ( final data will be present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1475312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "in which format are you getting the data :\n",
    "\n",
    "CSV format we are getting the from all the sources ( coma seperated value)\n",
    "\n",
    "JSON ------key value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77437060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8ae7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "Roles & Resonsibiities as a Data engineer :\n",
    "    \n",
    "    \n",
    "    1. understanding the Business requirements and getting to the know the data sources\n",
    "    \n",
    "    2. creating the tables as per the client requirements\n",
    "    \n",
    "    3. creating the views as per the client requirements\n",
    "    \n",
    "    4. handling the NUll records\n",
    "     \n",
    "    5. Handling the duplicate records\n",
    "    \n",
    "    6. maintaing the data qulaity checks\n",
    "    \n",
    "    7. scheduling the data refresh jobs by using airflow\n",
    "    \n",
    "    8. involving  in the data ingestion\n",
    "    \n",
    "    9. doing the data transformation as per the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdf36f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab59e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. what are the challenges you have faced while working as a data engineer .....? 7 - 8 challenges you have to notedown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f293099f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db36bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv interview questions :\n",
    "    \n",
    "    Adidas :\n",
    "    \n",
    "    \n",
    "    1. can you pls tell me the table names that you have created in the project ...?\n",
    "    \n",
    "    1. Sales table ( sales table)\n",
    "    \n",
    "    2.revenue table\n",
    "    \n",
    "    3. apparles table\n",
    "    \n",
    "    4. retail unit table\n",
    "    \n",
    "    5. afflitates table\n",
    "    \n",
    "    6. discounts tables\n",
    "    \n",
    "    \n",
    "    \n",
    "    2. can you pls tell me view names that you have created in the project  ---?\n",
    "    \n",
    "    \n",
    "    \n",
    "    discounts_view\n",
    "    \n",
    "    retial_sales_view\n",
    "    \n",
    "    Revenue_view\n",
    "    \n",
    "    \n",
    "    \n",
    "    3. can you write a sample spark code , that you have written in the project --?\n",
    "    \n",
    "    \n",
    "    databricks =====> spark.session.setapp()\n",
    "    \n",
    "    4. can you tell me some of the use case that you have worked in the project...?\n",
    "    \n",
    "    \n",
    "    use_cases : working on the data ingestion\n",
    "        \n",
    "    2. handling the null records in the tables\n",
    "    \n",
    "    3. woring on the data transformation and get the insights from it.\n",
    "    \n",
    "    \n",
    "    \n",
    "    5. can you explain where have you used python in the project....?\n",
    "    \n",
    "    pyspark dataframe\n",
    "    \n",
    "    you have custom function in python for handling the null records\n",
    "    \n",
    "    you have created cretain clases for the data ingestion activity...\n",
    "    \n",
    "    \n",
    "    \n",
    "    6. what is your daily volume of data that you are processing...? ( 2GB - 5 GB)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    7. what is your total size of the data in your project....?( 4 PB - 6 PB)  ====> KB ---> MB ---> GB ---TB ---PB --ZB\n",
    "    \n",
    "    \n",
    "    \n",
    "    8. can you write the sample spark-submit command full syntax ...?\n",
    "    \n",
    "    \n",
    "    \n",
    "    9. can you tell me what all the joins you have used in the project....?\n",
    "    \n",
    "    \n",
    "    \n",
    "    10.how are you the getting the work assigned....? ( JIRA , Agile and confluence) ===> Agile \n",
    "    \n",
    "    Scrum methodaly ------> JIRA Board -----> user story\n",
    "    \n",
    "    sprints ====> 2 weeks\n",
    "    \n",
    "    4 user stories ====> 4 tasks ( data ingestion , data transformation , data testing)\n",
    "     \n",
    "    \n",
    "    \n",
    "    11. how many team members are there in your project....? ( 4 - 5 members)\n",
    "    \n",
    "    2 data engineer , 1 lead and one manager\n",
    "    \n",
    "    \n",
    "    \n",
    "    12. can you handle the tasks independently.....? ( yes ---> once i ge the KT from )\n",
    "    \n",
    "    13. how are you maintaing the technical and Business documentation....?\n",
    "    \n",
    "    confluence \n",
    "    \n",
    "    \n",
    "    14. what code versioing tool that you are woring up on ( BITBUcket ( GITHUB))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
